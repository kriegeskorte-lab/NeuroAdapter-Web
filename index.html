<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Towards Interpretable Visual Decoding with Attention to Brain Representations">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Brain Decoding, fMRI, Latent Diffusion Models, Vision, Interpretability">
  <!-- TODO: List all authors -->
  <meta name="author" content="Pinyuan Feng, Hossein Adeli, Wenxuan Guo, Fan Cheng, Ethan Hwang, Nikolas Kriegeskorte">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Zuckerman Mind Brain Behavior Institute,Columbia University in the City of New York">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Towards Interpretable Visual Decoding with Attention to Brain Representations">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Towards Interpretable Visual Decoding with Attention to Brain Representations">
  <meta name="citation_author" content="Feng, Pinyuan">
  <meta name="citation_author" content="Adeli, Hossein">
  <meta name="citation_author" content="Guo, Wenxuan">
  <meta name="citation_author" content="Cheng, Fan">
  <meta name="citation_author" content="Hwang, Ethan">
  <meta name="citation_author" content="Kriegeskorte, Nikolas">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>PAPER_TITLE - AUTHOR_NAMES | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/NA.ico">
  <link rel="apple-touch-icon" href="static/images/NA.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>Related Papers from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="https://arxiv.org/abs/2505.17329" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Transformer brain encoders explain human high-level visual responses</h5>
            <span class="work-venue">NeurIPS 2025 - Spotlight (top %3)</span>
            <br>
            <span class="work-venue">Under Camera Ready Submission</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://2025.ccneuro.org/abstract_pdf/Hwang_2025_Discovering_visual_categorical_selectivity_across_whole.pdf" class="work-item" target="_blank">
          <div class="work-info">
            <h5>In Silico Mapping of Visual Categorical Selectivity Across the Whole Brain</h5>
            <span class="work-venue">NeurIPS 2025</span>
            <br>
            <span class="work-venue">Under Camera Ready Submission</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">Towards Interpretable Visual Decoding with Attention to Brain Representations</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://kriegeskortelab.zuckermaninstitute.columbia.edu/people/pinyuan-feng" target="_blank">Pinyuan Feng</a>,
              </span>
              <span class="author-block">
                  <a href="https://hosseinadeli.github.io/" target="_blank">Hossein Adeli</a>,
              </span>
              <span class="author-block">
                    <a href="https://kriegeskortelab.zuckermaninstitute.columbia.edu/people/wenxuan-guo" target="_blank">Wenxuan Guo</a>,
              </span>
              <span class="author-block">
                    <a href="https://kriegeskortelab.zuckermaninstitute.columbia.edu/people/fan-cheng" target="_blank">Fan Cheng</a>,
              </span>
              <span class="author-block">
                    <a href="https://kriegeskortelab.zuckermaninstitute.columbia.edu/people/ethan-hwang" target="_blank">Ethan Hwang</a>,
              </span>
              <span class="author-block">
                    <a href="https://kriegeskortelab.zuckermaninstitute.columbia.edu/people/nikolaus-kriegeskorte" target="_blank">Nikolaus Kriegeskorte</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your institution and conference/journal info -->
              <span class="author-block">
                <img src="static/images/VI.ico" alt="VI icon" class="inline-icon">
                <a href="https://kriegeskortelab.zuckermaninstitute.columbia.edu/" target="_blank">
                  Visual Inference Lab
                </a>
                <br>
                <a href="https://www.zuckermaninstitute.columbia.edu/" target="_blank">
                  Zuckerman Mind Brain Behavior Institute
                </a>
                <br>
                <img src="static/images/CU.ico" alt="CU icon" class="inline-icon">
                Columbia University in the City of New York
              </span>
            </div>
                
            <div class="column has-text-centered">
              <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
                    
              <!-- TODO: Add your supplementary material PDF or remove this section
              <span class="link-block">
                <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span> -->

              <!-- TODO: Replace with your GitHub repository URL -->
              <span class="link-block">
                <a href="https://github.com/kriegeskorte-lab/NeuroAdapter" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code to be released soon</span>
                </a>
              </span>

            </div>

            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block">
                <img src="static/images/UP.ico" alt="UP icon" class="inline-icon">
                <b>News: </b> Our short version paper has been accepted to <a href="https://brainbodyfm-workshop.github.io/#home">Foundation Models for the Brain and Body Workshop</a> @ NeurIPS 2025!
              </span>
            </div> -->

            <div style="width:80%; margin:1.5em auto; padding:1em 1.5em; font-size: 1.1rem; background:#fff; border-radius:8px; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
              <span class="author-block">
                <img src="static/images/UP.ico" alt="UP icon" class="inline-icon">
                <b>News:</b>
                <ul style="list-style-type:disc; margin:0.5em 0 0 1.5em; padding:0; text-align:left;">
                  <li>
                    09/30/2025: Our long version paper is now available on arXiv!
                  </li>
                  <li>
                    09/22/2025: Our short version paper has been accepted at
                    <a href="https://brainbodyfm-workshop.github.io/#home" target="_blank">
                      Foundation Models for the Brain and Body Workshop @ NeurIPS 2025</a> !
                  </li>
                  <li>
                    04/21/2025: Our <a href="https://2025.ccneuro.org/abstract_pdf/Adeli_2025_NeuroAdapter_Visual_Reconstruction_Masked_Brain_Representation.pdf" target="_blank">extended abstract</a> has been accepted at
                    <a href="https://2025.ccneuro.org/" target="_blank">
                      CCN 2025
                    </a>!
                  </li>
                </ul>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.png" alt="teaser" width="100%"/>
      <br>
      <br>
      <p style="font-size: 1.1rem;">
        <b>TL;DR:</b> Typical two-stage decoding pipelines first map brain activity to intermediate feature spaces (e.g., CLIP/DINO) and then use those embeddings to guide a generative model. Our end-to-end brain-to-image approach conditions a latent diffusion model directly on brain activity, enabling interpretations of the generative dynamics in both image and brain spaces.
      </p>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent work has demonstrated that complex visual stimuli can be decoded from human brain activity using deep generative models, helping brain science researchers interpret how the brain represents real-world scenes. However, most current approaches leverage mapping brain signals into intermediate image or text feature spaces before guiding the generative process, masking the effect of contributions from different brain areas on the final reconstruction output. In this work, we propose <i>NeuroAdapter</i>, a visual decoding framework that directly conditions a latent diffusion model on brain representations, bypassing the need for intermediate feature spaces. Our method demonstrates competitive visual reconstruction quality on public fMRI datasets compared to prior work, while providing greater transparency into how brain signals shape the generation process. To this end, we contribute an Image-Brain BI-directional interpretability framework (<i>IBBI </i>) which investigates cross-attention mechanisms across diffusion denoising steps to reveal how different cortical areas influence the unfolding generative trajectory. Our results highlight the potential of end-to-end brain-to-image decoding and establish a path toward interpreting diffusion models through the lens of visual neuroscience.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Stimuli carousel -->
<section class="hero is-small">
    <div class="hero-body">
        <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered" style="margin-top: 2rem;">Decoded Examples from Different NSD Subjects</h2>
        
        <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
                <img src="static/images/1_1.png"/>
                <h3 class="subtitle has-text-centered">
                    <div style="font-size: 1rem; margin-top: 10px;">
                        50 stimulus-prediction pairs are randomly selected from the test set of NSD Subject 1.
                    </div>
            </div>
            <div class="item">
                <img src="static/images/2_1.png"/>
                <h3 class="subtitle has-text-centered">
                    <div style="font-size: 1rem; margin-top: 10px;">
                        50 stimulus-prediction pairs are randomly selected from the test set of NSD Subject 2.
                    </div>
            </div>
            <div class="item">
                <img src="static/images/5_1.png"/>
                <h3 class="subtitle has-text-centered">
                    <div style="font-size: 1rem; margin-top: 10px;">
                        50 stimulus-prediction pairs are randomly selected from the test set of NSD Subject 5.
                    </div>
            </div>
            <div class="item">
                <img src="static/images/7_1.png"/>
                <h3 class="subtitle has-text-centered">
                    <div style="font-size: 1rem; margin-top: 10px;">
                        50 stimulus-prediction pairs are randomly selected from the test set of NSD Subject 7.
                    </div>
            </div>
        </div>
        </div>
    </div>
  </section>
<!-- End Stimuli carousel -->


     
<!--BibTex citation -->
  <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code">
          <code>@article{feng2025neuroadapter,
            title={Your Paper Title Here},
            author={First Author and Second Author and Third Author},
            journal={Conference/Journal Name},
            year={2024},
            url={https://your-domain.com/your-project-page}
          }</code></pre>
    </div>
  </section>
<!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
